{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "deaeebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "195b44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6d6f03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_reviews.fileids()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a536148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg'],\n",
       " [['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...], 'neg']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append([movie_reviews.words(fileid), category])\n",
    "documents[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a750bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(documents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "244776aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents = documents[0:1500]\n",
    "testing_documents = documents[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1050abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punc = list(string.punctuation)         \n",
    "all_words = []\n",
    "stop = stop + punc                    \n",
    "for doc in training_documents:\n",
    "    for w in doc[0]:\n",
    "        if w.lower() not in stop:\n",
    "            all_words.append(w.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f0eefa5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532462"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e713826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "dist = nltk.FreqDist(all_words)    \n",
    "features = dist.most_common(100000) \n",
    "feature_words = [i[0] for i in features]\n",
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ef64d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(document,stop):\n",
    "    punc = list(string.punctuation)          \n",
    "    words = []\n",
    "    \n",
    "    stop = stop + punc                    \n",
    "    for i in document:\n",
    "        if i.lower() not in stop:\n",
    "            words.append(i.lower())\n",
    "  \n",
    "    feature = {}\n",
    "    \n",
    "    for w in feature_words:\n",
    "        feature[w] = 0\n",
    "        \n",
    "    for w in feature_words:\n",
    "        if((w in words)):\n",
    "            feature[w]=feature[w]+1\n",
    "           \n",
    "    return feature             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6747e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[get_features(i[0],stop), i[1]] for i in training_documents]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "492c9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = [[get_features(i[0],stop), i[1]] for i in testing_documents]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "49c12c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier                      \n",
    "from  sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fc4ead3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC())>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_sklearn = SklearnClassifier(SVC())\n",
    "classifier_sklearn.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cf7e360f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9973333333333333"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a91a7b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.836"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn, testing_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
